{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hz8nAme9VDunBk_BArs_A3jeGPKV-GaZ",
      "authorship_tag": "ABX9TyMWdGe+QEwNLWCbmFLLJneS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejas163/Deep_Learning/blob/main/SGDinC%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x3-MGrldFZ0"
      },
      "outputs": [],
      "source": [
        "#include <pybind11/pybind11.h>\n",
        "#include <pybind11/numpy.h>\n",
        "#include <cmath>\n",
        "#include <iostream>\n",
        "\n",
        "namespace py = pybind11;\n",
        "\n",
        "\n",
        "void softmax_regression_epoch_cpp(const float *X, const unsigned char *y,\n",
        "\t\t\t\t\t\t\t\t  float *theta, size_t m, size_t n, size_t k,\n",
        "\t\t\t\t\t\t\t\t  float lr, size_t batch)\n",
        "{\n",
        "    /**\n",
        "     * A C++ version of the softmax regression epoch code.  This should run a\n",
        "     * single epoch over the data defined by X and y (and sizes m,n,k), and\n",
        "     * modify theta in place.  Your function will probably want to allocate\n",
        "     * (and then delete) some helper arrays to store the logits and gradients.\n",
        "     *\n",
        "     * Args:\n",
        "     *     X (const float *): pointer to X data, of size m*n, stored in row\n",
        "     *          major (C) format\n",
        "     *     y (const unsigned char *): pointer to y data, of size m\n",
        "     *     theta (float *): pointer to theta data, of size n*k, stored in row\n",
        "     *          major (C) format\n",
        "     *     m (size_t): number of examples\n",
        "     *     n (size_t): input dimension\n",
        "     *     k (size_t): number of classes\n",
        "     *     lr (float): learning rate / SGD step size\n",
        "     *     batch (int): SGD minibatch size\n",
        "     *\n",
        "     * Returns:\n",
        "     *     (None)\n",
        "     */\n",
        "\n",
        "    /// BEGIN YOUR CODE\n",
        "\n",
        "    /// END YOUR CODE\n",
        "}\n",
        "\n",
        "\n",
        "/**\n",
        " * This is the pybind11 code that wraps the function above.  It's only role is\n",
        " * wrap the function above in a Python module, and you do not need to make any\n",
        " * edits to the code\n",
        " */\n",
        "PYBIND11_MODULE(simple_ml_ext, m) {\n",
        "    m.def(\"softmax_regression_epoch_cpp\",\n",
        "    \t[](py::array_t<float, py::array::c_style> X,\n",
        "           py::array_t<unsigned char, py::array::c_style> y,\n",
        "           py::array_t<float, py::array::c_style> theta,\n",
        "           float lr,\n",
        "           int batch) {\n",
        "        softmax_regression_epoch_cpp(\n",
        "        \tstatic_cast<const float*>(X.request().ptr),\n",
        "            static_cast<const unsigned char*>(y.request().ptr),\n",
        "            static_cast<float*>(theta.request().ptr),\n",
        "            X.request().shape[0],\n",
        "            X.request().shape[1],\n",
        "            theta.request().shape[1],\n",
        "            lr,\n",
        "            batch\n",
        "           );\n",
        "    },\n",
        "    py::arg(\"X\"), py::arg(\"y\"), py::arg(\"theta\"),\n",
        "    py::arg(\"lr\"), py::arg(\"batch\"));\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pybind11\n",
        "!pip3 install numdifftools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5st_Rt2fe4s",
        "outputId": "344804a6-beb1-4604-8e43-412877c341f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybind11\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/243.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/243.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-2.13.6\n",
            "Collecting numdifftools\n",
            "  Downloading numdifftools-0.9.41-py2.py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.11/dist-packages (from numdifftools) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.8 in /usr/local/lib/python3.11/dist-packages (from numdifftools) (1.13.1)\n",
            "Downloading numdifftools-0.9.41-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numdifftools\n",
            "Successfully installed numdifftools-0.9.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "include <pybind11/pybind11.h>\n",
        "include <pybind11/numpy.h>\n",
        "include <cmath>\n",
        "include <iostream>\n",
        "\n",
        "namespace py = pybind11;\n",
        "\n",
        "void softmax_regression_epoch_cpp(const float *X, const unsigned char *y,\n",
        "\t\t\t\t\t\t\t\t  float *theta, size_t m, size_t n, size_t k,\n",
        "\t\t\t\t\t\t\t\t  float lr, size_t batch)\n",
        "{\n",
        "    /**\n",
        "     * A C++ version of the softmax regression epoch code.  This should run a\n",
        "     * single epoch over the data defined by X and y (and sizes m,n,k), and\n",
        "     * modify theta in place.  Your function will probably want to allocate\n",
        "     * (and then delete) some helper arrays to store the logits and gradients.\n",
        "     *\n",
        "     * Args:\n",
        "     *     X (const float *): pointer to X data, of size m*n, stored in row\n",
        "     *          major (C) format\n",
        "     *     y (const unsigned char *): pointer to y data, of size m\n",
        "     *     theta (float *): pointer to theta data, of size n*k, stored in row\n",
        "     *          major (C) format\n",
        "     *     m (size_t): number of examples\n",
        "     *     n (size_t): input dimension\n",
        "     *     k (size_t): number of classes\n",
        "     *     lr (float): learning rate / SGD step size\n",
        "     *     batch (int): SGD minibatch size\n",
        "     *\n",
        "     * Returns:\n",
        "     *     (None)\n",
        "     */\n",
        "\n",
        "    float *logits = new float[m * k];\n",
        "    float *gradients = new float[n * k];\n",
        "\n",
        "    for (size_t i = 0; i < m; ++i) {\n",
        "        for (size_t j = 0; j < k; ++j) {\n",
        "            logits[i * k + j] = 0;\n",
        "            for (size_t l = 0; l < n; ++l) {\n",
        "                logits[i * k + j] += X[i * n + l] * theta[l * k + j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (size_t i = 0; i < n * k; ++i)\n",
        "        gradients[i] = 0;\n",
        "\n",
        "    for (size_t i = 0; i < m; i++) {\n",
        "        float sum = 0;\n",
        "        for (size_t j = 0; j < k; j++) {\n",
        "          sum += exp(logits[i * k + j]);\n",
        "        }\n",
        "\n",
        "        for (size_t j = 0; j < n; j++) {\n",
        "          for(size_t l = 0; l < k; l++){\n",
        "            float prob = exp(logits[i * k + l]) / sum;\n",
        "            gradients[j * k + l] += (prob - (l == y[i])) * X[i * n + j];\n",
        "          }\n",
        "        }\n",
        "    }\n",
        "    for(size_t i = 0; i < n * k; i++) {\n",
        "        theta[i] -= lr * gradients[i] / m;\n",
        "    }\n",
        "    delete[] logits;\n",
        "    delete[] gradients;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "/**\n",
        " * This is the pybind11 code that wraps the function above.  It's only role is\n",
        " * wrap the function above in a Python module, and you do not need to make any\n",
        " * edits to the code\n",
        " */\n",
        "PYBIND11_MODULE(simple_ml_ext, m) {\n",
        "    m.def(\"softmax_regression_epoch_cpp\",\n",
        "    \t[](py::array_t<float, py::array::c_style> X,\n",
        "           py::array_t<unsigned char, py::array::c_style> y,\n",
        "           py::array_t<float, py::array::c_style> theta,\n",
        "           float lr,\n",
        "           int batch) {\n",
        "        softmax_regression_epoch_cpp(\n",
        "        \tstatic_cast<const float*>(X.request().ptr),\n",
        "            static_cast<const unsigned char*>(y.request().ptr),\n",
        "            static_cast<float*>(theta.request().ptr),\n",
        "            X.request().shape[0],\n",
        "            X.request().shape[1],\n",
        "            theta.request().shape[1],\n",
        "            lr,\n",
        "            batch\n",
        "           );\n",
        "    },\n",
        "    py::arg(\"X\"), py::arg(\"y\"), py::arg(\"theta\"),\n",
        "    py::arg(\"lr\"), py::arg(\"batch\"));\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "RkpJGWsteyzd",
        "outputId": "d06abb97-85ab-4d91-e5fd-2024d115063f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-8288b3313131>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8288b3313131>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    namespace py = pybind11;\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}