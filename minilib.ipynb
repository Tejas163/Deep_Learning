{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Tejas163/Deep_Learning/blob/main/minilib.ipynb",
      "authorship_tag": "ABX9TyMrlM76peAKJ5TRtmrpStF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejas163/Deep_Learning/blob/main/minilib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing MNIST"
      ],
      "metadata": {
        "id": "YMU9x59FNkcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "import struct\n",
        "\n",
        "def parse_mnist(image_filename, label_filename):\n",
        "    \"\"\" Read an images and labels file in MNIST format.  See this page:\n",
        "    http://yann.lecun.com/exdb/mnist/ for a description of the file format.\n",
        "    Args:\n",
        "        image_filename (str): name of gzipped images file in MNIST format\n",
        "        label_filename (str): name of gzipped labels file in MNIST format\n",
        "    Returns:\n",
        "        Tuple (X,y):\n",
        "            X (numpy.ndarray[np.float32]): 2D numpy array containing the loaded\n",
        "                data.  The dimensionality of the data should be\n",
        "                (num_examples x input_dim) where 'input_dim' is the full\n",
        "                dimension of the data, e.g., since MNIST images are 28x28, it\n",
        "                will be 784.  Values should be of type np.float32, and the data\n",
        "                should be normalized to have a minimum value of 0.0 and a\n",
        "                maximum value of 1.0 (i.e., scale original values of 0 to 0.0\n",
        "                and 255 to 1.0).\n",
        "            y (numpy.ndarray[dtype=np.uint8]): 1D numpy array containing the\n",
        "                labels of the examples.  Values should be of type np.uint8 and\n",
        "                for MNIST will contain the values 0-9.\n",
        "    \"\"\"\n",
        "    with gzip.open(image_filename, 'rb') as f:\n",
        "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
        "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows * cols)\n",
        "        images = images.astype(np.float32) / 255.0\n",
        "\n",
        "    with gzip.open(label_filename, 'rb') as f:\n",
        "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
        "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "ggdqrgRzOKaL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = parse_mnist('/content/drive/MyDrive/10714/hw0/data/t10k-images-idx3-ubyte.gz', '/content/drive/MyDrive/10714/hw0/data/t10k-labels-idx1-ubyte.gz')\n",
        "print(\"Images shape:\", images.shape)  # Should be (60000, 784) for the training set\n",
        "print(\"Labels shape:\", labels.shape)  # Should be (60000,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDNDwtxXQdjE",
        "outputId": "6795ad84-2149-4443-c06c-6ed8c1c0080b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: (10000, 784)\n",
            "Labels shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement the Softmax loss function"
      ],
      "metadata": {
        "id": "daaoc8d9aflu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_loss(Z, y):\n",
        "    \"\"\" Return softmax loss.  Note that for the purposes of this assignment,\n",
        "    you don't need to worry about \"nicely\" scaling the numerical properties\n",
        "    of the log-sum-exp computation, but can just compute this directly.\n",
        "    Args:\n",
        "        Z (np.ndarray[np.float32]): 2D numpy array of shape\n",
        "            (batch_size, num_classes), containing the logit predictions for\n",
        "            each class.\n",
        "        y (np.ndarray[np.uint8]): 1D numpy array of shape (batch_size, )\n",
        "            containing the true label of each example.\n",
        "    Returns:\n",
        "        Average softmax loss over the sample.\"\"\"\n",
        "\n",
        "    Z -= np.max(Z, axis=1, keepdims=True) #For numerical stability\n",
        "    exp_scores = np.exp(Z)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    N = Z.shape[0]\n",
        "    corect_logprobs = -np.log(probs[range(N),y])\n",
        "    data_loss = np.sum(corect_logprobs)/N\n",
        "    return data_loss"
      ],
      "metadata": {
        "id": "qqy7uq2iairA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_softmax_loss():\n",
        "    X,y = parse_mnist(\"/content/drive/MyDrive/10714/hw0/data/t10k-images-idx3-ubyte.gz\",\n",
        "                      \"/content/drive/MyDrive/10714/hw0/data/t10k-labels-idx1-ubyte.gz\")\n",
        "    np.random.seed(0)\n",
        "\n",
        "    Z = np.zeros((y.shape[0], 10))\n",
        "    np.testing.assert_allclose(softmax_loss(Z,y), 2.3025850)\n",
        "    Z = np.random.randn(y.shape[0], 10)\n",
        "    calculated_loss = softmax_loss(Z,y)\n",
        "    print(f\"Calculated loss:{calculated_loss}, expected loss: 2.3025850\")\n",
        "    np.testing.assert_allclose(softmax_loss(Z,y), 2.7291998)"
      ],
      "metadata": {
        "id": "M7Mx4in-dNPo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_softmax_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "qljzNPpAbNPv",
        "outputId": "a9ba0b82-9452-462f-c986-a388d7072819"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated loss:2.7328716716962513, expected loss: 2.3025850\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "\nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 1 / 1 (100%)\nMax absolute difference: 0.00367187\nMax relative difference: 0.0013454\n x: array(2.732872)\n y: array(2.7292)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fa02fb955aa4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_softmax_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-622436cb81ce>\u001b[0m in \u001b[0;36mtest_softmax_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcalculated_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Calculated loss:{calculated_loss}, expected loss: 2.3025850\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.7291998\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    795\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 1 / 1 (100%)\nMax absolute difference: 0.00367187\nMax relative difference: 0.0013454\n x: array(2.732872)\n y: array(2.7292)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax_regression_epoch(X, y, theta, lr = 0.1, batch=100):\n",
        "    \"\"\" Run a single epoch of SGD for softmax regression on the data, using\n",
        "    the step size lr and specified batch size.  This function should modify the\n",
        "    theta matrix in place, and you should iterate through batches in X _without_\n",
        "    randomizing the order.\n",
        "    Args:\n",
        "        X (np.ndarray[np.float32]): 2D input array of size\n",
        "            (num_examples x input_dim).\n",
        "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
        "        theta (np.ndarrray[np.float32]): 2D array of softmax regression\n",
        "            parameters, of shape (input_dim, num_classes)\n",
        "        lr (float): step size (learning rate) for SGD\n",
        "        batch (int): size of SGD minibatch\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    num_examples = X.shape[0]\n",
        "    num_classes = theta.shape[1]\n",
        "\n",
        "    for i in range(0, num_examples, batch):\n",
        "        X_batch = X[i: i + batch]\n",
        "        y_batch = y[i: i + batch]\n",
        "\n",
        "        Z = X_batch @ theta\n",
        "        Z -= np.max(Z, axis = 1, keepdims = True)\n",
        "        exp_scores = np.exp(Z)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True)\n",
        "\n",
        "        # Compute the gradient\n",
        "        delta = probs\n",
        "        delta[range(X_batch.shape[0]), y_batch] -= 1\n",
        "        grad = (X_batch.T @ delta) / X_batch.shape[0]\n",
        "\n",
        "        theta -= lr * grad\n"
      ],
      "metadata": {
        "id": "WDT84uxsfhbh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def nn_epoch(X, y, W1, W2, lr = 0.1, batch=100):\n",
        "    \"\"\" Run a single epoch of SGD for a two-layer neural network defined by the\n",
        "    weights W1 and W2 (with no bias terms):\n",
        "        logits = ReLU(X * W1) * W2\n",
        "    The function should use the step size lr, and the specified batch size (and\n",
        "    again, without randomizing the order of X).  It should modify the\n",
        "    W1 and W2 matrices in place.\n",
        "    Args:\n",
        "        X (np.ndarray[np.float32]): 2D input array of size\n",
        "            (num_examples x input_dim).\n",
        "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
        "        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape\n",
        "            (input_dim, hidden_dim)\n",
        "        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape\n",
        "            (hidden_dim, num_classes)\n",
        "        lr (float): step size (learning rate) for SGD\n",
        "        batch (int): size of SGD minibatch\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    num_examples = X.shape[0]\n",
        "    hidden_dim = W1.shape[1]\n",
        "    num_classes = W2.shape[1]\n",
        "\n",
        "    for i in range(0, num_examples, batch):\n",
        "        X_batch = X[i:i + batch]\n",
        "        y_batch = y[i:i + batch]\n",
        "\n",
        "        # Forward pass\n",
        "        hidden_layer = np.maximum(0, X_batch @ W1)  # ReLU activation\n",
        "        logits = hidden_layer @ W2\n",
        "\n",
        "        # Softmax\n",
        "        logits -= np.max(logits, axis=1, keepdims=True)\n",
        "        exp_scores = np.exp(logits)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # Backpropagation\n",
        "        delta3 = probs\n",
        "        delta3[range(X_batch.shape[0]), y_batch] -= 1\n",
        "        dW2 = (hidden_layer.T @ delta3) / X_batch.shape[0]\n",
        "\n",
        "        delta2 = (delta3 @ W2.T) * (hidden_layer > 0) #derivative of ReLU\n",
        "        dW1 = (X_batch.T @ delta2) / X_batch.shape[0]\n",
        "\n",
        "        # Update weights\n",
        "        W1 -= lr * dW1\n",
        "        W2 -= lr * dW2\n"
      ],
      "metadata": {
        "id": "o-FWq4MoLUI_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_err(h,y):\n",
        "    \"\"\" Helper funciton to compute both loss and error\"\"\"\n",
        "    return softmax_loss(h,y), np.mean(h.argmax(axis=1) != y)"
      ],
      "metadata": {
        "id": "KkemnAe8MBhH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr=0.5, batch=100,\n",
        "                  cpp=False):\n",
        "    \"\"\" Example function to fully train a softmax regression classifier \"\"\"\n",
        "    theta = np.zeros((X_tr.shape[1], y_tr.max()+1), dtype=np.float32)\n",
        "    print(\"| Epoch | Train Loss | Train Err | Test Loss | Test Err |\")\n",
        "    for epoch in range(epochs):\n",
        "        if not cpp:\n",
        "            softmax_regression_epoch(X_tr, y_tr, theta, lr=lr, batch=batch)\n",
        "        else:\n",
        "            softmax_regression_epoch_cpp(X_tr, y_tr, theta, lr=lr, batch=batch)\n",
        "        train_loss, train_err = loss_err(X_tr @ theta, y_tr)\n",
        "        test_loss, test_err = loss_err(X_te @ theta, y_te)\n",
        "        print(\"|  {:>4} |    {:.5f} |   {:.5f} |   {:.5f} |  {:.5f} |\"\\\n",
        "              .format(epoch, train_loss, train_err, test_loss, test_err))"
      ],
      "metadata": {
        "id": "8B8bDFNBL42c"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr, y_tr = parse_mnist(\"/content/drive/MyDrive/10714/hw0/data/train-images-idx3-ubyte.gz\",\n",
        "                         \"/content/drive/MyDrive/10714/hw0/data/train-labels-idx1-ubyte.gz\")\n",
        "X_te, y_te = parse_mnist(\"/content/drive/MyDrive/10714/hw0/data/t10k-images-idx3-ubyte.gz\",\n",
        "                         \"/content/drive/MyDrive/10714/hw0/data/t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr=0.2, batch=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KidMMRTsbBQ3",
        "outputId": "1005231b-54af-4133-dc26-6311c8ab04aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
            "|     0 |    0.35134 |   0.10182 |   0.33588 |  0.09400 |\n",
            "|     1 |    0.32142 |   0.09268 |   0.31086 |  0.08730 |\n",
            "|     2 |    0.30802 |   0.08795 |   0.30097 |  0.08550 |\n",
            "|     3 |    0.29987 |   0.08532 |   0.29558 |  0.08370 |\n",
            "|     4 |    0.29415 |   0.08323 |   0.29215 |  0.08230 |\n",
            "|     5 |    0.28981 |   0.08182 |   0.28973 |  0.08090 |\n",
            "|     6 |    0.28633 |   0.08085 |   0.28793 |  0.08080 |\n",
            "|     7 |    0.28345 |   0.07997 |   0.28651 |  0.08040 |\n",
            "|     8 |    0.28100 |   0.07923 |   0.28537 |  0.08010 |\n",
            "|     9 |    0.27887 |   0.07847 |   0.28442 |  0.07970 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn(X_tr, y_tr, X_te, y_te, hidden_dim = 500,\n",
        "             epochs=10, lr=0.5, batch=100):\n",
        "    \"\"\" Example function to train two layer neural network \"\"\"\n",
        "    n, k = X_tr.shape[1], y_tr.max() + 1\n",
        "    np.random.seed(0)\n",
        "    W1 = np.random.randn(n, hidden_dim).astype(np.float32) / np.sqrt(hidden_dim)\n",
        "    W2 = np.random.randn(hidden_dim, k).astype(np.float32) / np.sqrt(k)\n",
        "\n",
        "    print(\"| Epoch | Train Loss | Train Err | Test Loss | Test Err |\")\n",
        "    for epoch in range(epochs):\n",
        "        nn_epoch(X_tr, y_tr, W1, W2, lr=lr, batch=batch)\n",
        "        train_loss, train_err = loss_err(np.maximum(X_tr@W1,0)@W2, y_tr)\n",
        "        test_loss, test_err = loss_err(np.maximum(X_te@W1,0)@W2, y_te)\n",
        "        print(\"|  {:>4} |    {:.5f} |   {:.5f} |   {:.5f} |  {:.5f} |\"\\\n",
        "              .format(epoch, train_loss, train_err, test_loss, test_err))\n"
      ],
      "metadata": {
        "id": "H3Y6qTOwbliD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr, y_tr = parse_mnist(\"/content/drive/MyDrive/10714/hw0/data/train-images-idx3-ubyte.gz\",\n",
        "                         \"/content/drive/MyDrive/10714/hw0/data/train-labels-idx1-ubyte.gz\")\n",
        "X_te, y_te = parse_mnist(\"/content/drive/MyDrive/10714/hw0/data/t10k-images-idx3-ubyte.gz\",\n",
        "                         \"/content/drive/MyDrive/10714/hw0/data/t10k-labels-idx1-ubyte.gz\")\n",
        "train_nn(X_tr, y_tr, X_te, y_te, hidden_dim=400, epochs=20, lr=0.2)"
      ],
      "metadata": {
        "id": "nBaAwuH7bvgm",
        "outputId": "764e51e7-5a21-46a0-8aeb-4201ffe4c1d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
            "|     0 |    0.15324 |   0.04697 |   0.16305 |  0.04920 |\n",
            "|     1 |    0.09854 |   0.02923 |   0.11604 |  0.03660 |\n",
            "|     2 |    0.07396 |   0.02143 |   0.09704 |  0.03130 |\n",
            "|     3 |    0.05930 |   0.01707 |   0.08757 |  0.02860 |\n",
            "|     4 |    0.04782 |   0.01315 |   0.08025 |  0.02550 |\n",
            "|     5 |    0.04056 |   0.01083 |   0.07687 |  0.02400 |\n",
            "|     6 |    0.03488 |   0.00913 |   0.07430 |  0.02360 |\n",
            "|     7 |    0.03028 |   0.00770 |   0.07242 |  0.02290 |\n",
            "|     8 |    0.02643 |   0.00637 |   0.07054 |  0.02180 |\n",
            "|     9 |    0.02355 |   0.00542 |   0.06972 |  0.02140 |\n",
            "|    10 |    0.02101 |   0.00465 |   0.06886 |  0.02110 |\n",
            "|    11 |    0.01884 |   0.00393 |   0.06791 |  0.02070 |\n",
            "|    12 |    0.01716 |   0.00322 |   0.06752 |  0.02100 |\n",
            "|    13 |    0.01556 |   0.00278 |   0.06694 |  0.02090 |\n",
            "|    14 |    0.01421 |   0.00237 |   0.06656 |  0.02090 |\n",
            "|    15 |    0.01282 |   0.00198 |   0.06615 |  0.02090 |\n",
            "|    16 |    0.01185 |   0.00182 |   0.06593 |  0.02020 |\n",
            "|    17 |    0.01088 |   0.00158 |   0.06564 |  0.01990 |\n",
            "|    18 |    0.00999 |   0.00132 |   0.06542 |  0.01990 |\n",
            "|    19 |    0.00914 |   0.00110 |   0.06496 |  0.01940 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
        "                             \"data/train-labels-idx1-ubyte.gz\")\n",
        "    X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
        "                             \"data/t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "    print(\"Training softmax regression\")\n",
        "    train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr = 0.1)\n",
        "\n",
        "    print(\"\\nTraining two layer neural network w/ 100 hidden units\")\n",
        "    train_nn(X_tr, y_tr, X_te, y_te, hidden_dim=100, epochs=20, lr = 0.2)\n"
      ],
      "metadata": {
        "id": "s0jWbzSPccZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh-_wcjEg-S3",
        "outputId": "317a0089-d4eb-4a7e-9531-e170d7fa7485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDArray(shape=(2, 3), strides=(1, 1), offset=0)\n",
            "NDArray(shape=(2, 3), strides=(1, 1), offset=0)\n",
            "NDArray(shape=(3, 2), strides=None, offset=0)\n",
            "NDArray(shape=(3, 2), strides=(1, 1), offset=0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NDArray(shape=(2, 3), strides=(1, 1), offset=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# there are five fields within the NDArray class that you'll need to be familiar with (note that the real class member these all these fields is preceded by an underscore, e.g., _handle, _strides, etc, some of which are then exposed as a public property ... for all your code it's fine to use the internal, underscored version).\n",
        "# device - A object of type BackendDevice, which is a simple wrapper that contains a link to the underlying device backend (e.g., CPU or CUDA).\n",
        "# handle - A class objected that stores the underlying memory of the array. This is allocated as a class of type device.Array(), though this allocation all happens in the provided code (specifically the NDArray.make function), and you don't need to worry about calling it yourself.\n",
        "# shape - A tuple specifying the size of each dimension in the array.\n",
        "# strides - A tuple specifying the strides of each dimension in the array.\n",
        "# offset - An integer indicating where in the underlying device.Array memory the array actually starts (it's convenient to store this so we can more easily manage pointing back to existing memory, without having to track allocations).\n",
        "# Of particular importance for many of your Python implementations will be the NDArray.make call:\n",
        "# def make(shape, strides=None, device=None, handle=None, offset=0):\n",
        "# which creates a new NDArray with the given shape, strides, device, handle, and offset. If handle is not specified (i.e., no pre-existing memory is referenced), then the call will allocate the needed memory, but if handle is specified then no new memory is allocated, but the new NDArray points the same memory as the old one\n",
        "\n",
        "class BackendDevice:\n",
        "    def __init__(self):\n",
        "        pass  # Replace with actual device initialization if needed\n",
        "\n",
        "\n",
        "class NDArray:\n",
        "    def __init__(self, device, handle, shape, strides, offset):\n",
        "        self._device = device\n",
        "        self._handle = handle\n",
        "        self._shape = shape\n",
        "        self._strides = strides\n",
        "        self._offset = offset\n",
        "\n",
        "    @classmethod\n",
        "    def make(cls, shape, strides=None, device=None, handle=None, offset=0):\n",
        "        if handle is None:\n",
        "            # Allocate new memory if handle is not provided.\n",
        "            # Replace with actual memory allocation based on device and shape\n",
        "            # For demonstration, using a placeholder\n",
        "            handle = object()\n",
        "\n",
        "        if device is None:\n",
        "            device = BackendDevice()\n",
        "\n",
        "        if strides is None:\n",
        "            # Calculate default strides if not given\n",
        "            strides = tuple([1] * len(shape))\n",
        "\n",
        "        return cls(device, handle, shape, strides, offset)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"NDArray(shape={self._shape}, strides={self._strides}, offset={self._offset})\"\n",
        "\n",
        "    def reshape(self, new_shape):\n",
        "      curr_size=1\n",
        "      for dim in self._shape:\n",
        "        curr_size*=dim\n",
        "\n",
        "      new_size=1\n",
        "      for dim in new_shape:\n",
        "        new_size*=dim\n",
        "\n",
        "      if curr_size!=new_size:\n",
        "          raise ValueError(\"Product of current shape is not equal to the product of the new shape.\")\n",
        "\n",
        "      return NDArray(self._device, self._handle, new_shape, None, self._offset)  # strides will be recalculated if needed\n",
        "\n",
        "    def permute(self, new_axes):\n",
        "        new_shape = tuple(self._shape[i] for i in new_axes)\n",
        "        new_strides = tuple(self._strides[i] for i in new_axes)\n",
        "\n",
        "        return NDArray(self._device, self._handle, new_shape, new_strides, self._offset)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "array = NDArray.make((2, 3))\n",
        "print(array)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "array = NDArray.make((2, 3))\n",
        "print(array)\n",
        "\n",
        "reshaped_array = array.reshape((3,2))\n",
        "print(reshaped_array)\n",
        "\n",
        "premute_array = array.permute((1,0))\n",
        "print(premute_array)\n",
        "\n",
        "# Creating an array referencing existing memory (demonstration)\n",
        "existing_array = NDArray.make((2, 3))\n",
        "new_array = NDArray.make((2, 3), handle=existing_array._handle)\n",
        "new_array"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part1"
      ],
      "metadata": {
        "id": "zg5-VSN1mgJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: def reshape(self, new_shape):\n",
        "#         \"\"\"\n",
        "#         Reshape the matrix without copying memory.  This will return a matrix\n",
        "#         that corresponds to a reshaped array but points to the same memory as\n",
        "#         the original array.\n",
        "#         Raises:\n",
        "#             ValueError if product of current shape is not equal to the product\n",
        "#             of the new shape, or if the matrix is not compact.\n",
        "#         Args:\n",
        "#             new_shape (tuple): new shape of the array\n",
        "#         Returns:\n",
        "#             NDArray : reshaped array; this will point to thep\n",
        "#         \"\"\"\n",
        "def reshape(self, new_shape):\n",
        "        \"\"\"\n",
        "        Reshape the matrix without copying memory.  This will return a matrix\n",
        "        that corresponds to a reshaped array but points to the same memory as\n",
        "        the original array.\n",
        "        Raises:\n",
        "            ValueError if product of current shape is not equal to the product\n",
        "            of the new shape, or if the matrix is not compact.\n",
        "        Args:\n",
        "            new_shape (tuple): new shape of the array\n",
        "        Returns:\n",
        "            NDArray : reshaped array; this will point to thep\n",
        "        \"\"\"\n",
        "        current_size = 1\n",
        "        for dim in self._shape:\n",
        "            current_size *= dim\n",
        "\n",
        "        new_size = 1\n",
        "        for dim in new_shape:\n",
        "            new_size *= dim\n",
        "\n",
        "        if current_size != new_size:\n",
        "            raise ValueError(\"Product of current shape is not equal to the product of the new shape.\")\n",
        "\n",
        "        #  Add check for compact matrix if needed\n",
        "        #  if not self.is_compact():\n",
        "        #    raise ValueError(\"The matrix is not compact.\")\n",
        "\n",
        "        return NDArray(self._device, self._handle, new_shape, None, self._offset)  # strides will be recalculated if needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "fRzgRi1bl4Qr",
        "outputId": "1ee5f1bc-f7f2-4bb9-e493-00211e4950f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDArray(shape=(2, 3), strides=(1, 1), offset=0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NDArray' object has no attribute 'reshape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dc089acf5b83>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mreshaped_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NDArray' object has no attribute 'reshape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permute method"
      ],
      "metadata": {
        "id": "YAQfy2K90J_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: def permute(self, new_axes):\n",
        "#         \"\"\"\n",
        "#         Permute order of the dimensions.  new_axes describes a permuation of the\n",
        "#         existing axes, so e.g.:\n",
        "#           - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\n",
        "#             would convert this to \"BCHW\" order.\n",
        "#           - For a 2D array, .permute((1,0)) would transpose the array.\n",
        "#         Like reshape, this operation should not copy memory, but achieves the\n",
        "#         permuting by just adjusting the shape/strides of the array.  That is,\n",
        "#         it returns a new array that has the dimensions permuted as desired, but\n",
        "#         which points to the same memroy as the original array.\n",
        "#         Args:\n",
        "#             new_axes (tuple): permuation order of the dimensions\n",
        "#         Returns:\n",
        "#             NDarray : new NDArray object with permuted dimensions, pointing\n",
        "#             to the same memory as the original NDArray (i.e., just shape and\n",
        "#             strides changed).\n",
        "#         \"\"\"\n",
        "\n",
        "def permute(self, new_axes):\n",
        "        \"\"\"\n",
        "        Permute order of the dimensions.  new_axes describes a permuation of the\n",
        "        existing axes, so e.g.:\n",
        "          - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\n",
        "            would convert this to \"BCHW\" order.\n",
        "          - For a 2D array, .permute((1,0)) would transpose the array.\n",
        "        Like reshape, this operation should not copy memory, but achieves the\n",
        "        permuting by just adjusting the shape/strides of the array.  That is,\n",
        "        it returns a new array that has the dimensions permuted as desired, but\n",
        "        which points to the same memroy as the original array.\n",
        "        Args:\n",
        "            new_axes (tuple): permuation order of the dimensions\n",
        "        Returns:\n",
        "            NDarray : new NDArray object with permuted dimensions, pointing\n",
        "            to the same memory as the original NDArray (i.e., just shape and\n",
        "            strides changed).\n",
        "        \"\"\"\n",
        "\n",
        "        new_shape = tuple(self._shape[i] for i in new_axes)\n",
        "        new_strides = tuple(self._strides[i] for i in new_axes)\n",
        "\n",
        "        return NDArray(self._device, self._handle, new_shape, new_strides, self._offset)"
      ],
      "metadata": {
        "id": "nsBDsQUv0OSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}